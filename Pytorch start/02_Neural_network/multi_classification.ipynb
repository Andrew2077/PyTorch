{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED =42 \n",
    "N_SAMPLES = 1000\n",
    "\n",
    "\n",
    "x_blob , y_blob  = make_blobs(\n",
    "    n_samples = N_SAMPLES,\n",
    "    n_features= NUM_FEATURES,\n",
    "    centers = NUM_CLASSES,\n",
    "    cluster_std= 1.5,\n",
    "    random_state=RANDOM_SEED    \n",
    ")\n",
    "\n",
    "x_blob = torch.from_numpy(x_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "\n",
    "#* train test split can still work with tensors\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    x_blob, y_blob, test_size = 0.2, random_state = RANDOM_SEED\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class BlobModel(nn.Module):\n",
    "    def __init__(self, input_features, n_classes, n_neurons = 10) -> None:\n",
    "        super().__init__()\n",
    "        self.Linear_block = nn.Sequential(\n",
    "            nn.Linear(input_features, n_neurons),\n",
    "            nn.Linear(n_neurons, n_neurons),\n",
    "            nn.Linear(n_neurons, n_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.Linear_block(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss & optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlobModel(input_features=NUM_FEATURES, \n",
    "                    n_classes=NUM_CLASSES, \n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = blob_model(\n",
    "#     n_classes= 4, \n",
    "#     input_features=2,\n",
    "#     hidden_units=8\n",
    "# )\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params= model.parameters(),lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fun(y_preds, y_true):\n",
    "    return (torch.eq(y_preds, y_true).sum()/len(ytrain)*100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    #* torch.eq => comparison between tensor, returns boolean\n",
    "    correct = torch.eq(y_true,y_pred).sum().item()\n",
    "    acc = correct/(len(y_pred))*100\n",
    "    return acc\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to make prediction for multiclasses \n",
    "- use softmax activation function\n",
    "  - predict with the model\n",
    "  - pass prediction to `torch.softmax()`\n",
    "  - pass the result to `torch.argmax()`\n",
    "  - don't forget to change the type to mac the type of the label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRaining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_blob , y_blob  = make_blobs(\n",
    "    n_samples = N_SAMPLES,\n",
    "    n_features= NUM_FEATURES,\n",
    "    centers = NUM_CLASSES,\n",
    "    cluster_std= 1.5,\n",
    "    random_state=RANDOM_SEED    \n",
    ")\n",
    "\n",
    "X_blob = torch.from_numpy(x_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/151 | Loss: 1.9090 | Acc: 25.00% | Test Loss: 1.6338 | Test Acc: 19.50%\n",
      "Epoch: 11/151 | Loss: 0.7898 | Acc: 66.88% | Test Loss: 0.7733 | Test Acc: 75.50%\n",
      "Epoch: 21/151 | Loss: 0.5454 | Acc: 95.25% | Test Loss: 0.5402 | Test Acc: 95.50%\n",
      "Epoch: 31/151 | Loss: 0.4243 | Acc: 97.25% | Test Loss: 0.4226 | Test Acc: 97.50%\n",
      "Epoch: 41/151 | Loss: 0.3498 | Acc: 97.75% | Test Loss: 0.3496 | Test Acc: 98.00%\n",
      "Epoch: 51/151 | Loss: 0.2979 | Acc: 98.38% | Test Loss: 0.2982 | Test Acc: 98.50%\n",
      "Epoch: 61/151 | Loss: 0.2592 | Acc: 98.88% | Test Loss: 0.2596 | Test Acc: 99.00%\n",
      "Epoch: 71/151 | Loss: 0.2292 | Acc: 99.00% | Test Loss: 0.2294 | Test Acc: 99.00%\n",
      "Epoch: 81/151 | Loss: 0.2052 | Acc: 99.00% | Test Loss: 0.2052 | Test Acc: 99.00%\n",
      "Epoch: 91/151 | Loss: 0.1857 | Acc: 99.12% | Test Loss: 0.1853 | Test Acc: 99.00%\n",
      "Epoch: 101/151 | Loss: 0.1695 | Acc: 99.12% | Test Loss: 0.1688 | Test Acc: 99.00%\n",
      "Epoch: 111/151 | Loss: 0.1559 | Acc: 99.12% | Test Loss: 0.1549 | Test Acc: 99.00%\n",
      "Epoch: 121/151 | Loss: 0.1443 | Acc: 99.12% | Test Loss: 0.1430 | Test Acc: 99.00%\n",
      "Epoch: 131/151 | Loss: 0.1344 | Acc: 99.00% | Test Loss: 0.1327 | Test Acc: 99.00%\n",
      "Epoch: 141/151 | Loss: 0.1258 | Acc: 99.00% | Test Loss: 0.1238 | Test Acc: 99.00%\n",
      "Epoch: 151/151 | Loss: 0.1183 | Acc: 99.00% | Test Loss: 0.1160 | Test Acc: 99.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "epochs = 151\n",
    "\n",
    "xtrain , ytrain = xtrain.to(device), ytrain.to(device)\n",
    "xtest, ytest = xtest.to(device), ytest.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    logits = model(xtrain)\n",
    "    y_preds = torch.softmax(logits, dim=1).argmax(dim=1)\n",
    "    loss = loss_fn(logits, ytrain)\n",
    "    acc = accuracy_fn(y_pred= y_preds, y_true= ytrain)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        logits = model(xtest)\n",
    "        y_preds = torch.softmax(logits, dim=1).argmax(dim=1)\n",
    "        loss_test = loss_fn(logits, ytest)\n",
    "        acc_test = accuracy_fn(y_pred= y_preds, y_true= ytest)\n",
    "        \n",
    "        if epoch % 10 == 0:    \n",
    "            print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss.item():.4f} | Acc: {acc:.2f}% | Test Loss: {loss_test.item():.4f} | Test Acc: {acc_test:.2f}%\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9c3c500b8bab2cdd86682acf135365680fb2fcb10592a31bb294351b9c146c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
