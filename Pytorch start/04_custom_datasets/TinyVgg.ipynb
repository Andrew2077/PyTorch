{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/images/train/', 'data/images/test/')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = 'data/'\n",
    "dir1 = os.listdir(directory)\n",
    "directory = directory + dir1[1] + '/'\n",
    "dir2 = os.listdir(directory)\n",
    "train_dir  = directory + dir2[1] + '/'\n",
    "test_dir = directory + dir2[0] + '/'\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import pathlib\n",
    "\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=Compose([Resize((224, 224)), ToTensor()]),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=Compose([Resize((224, 224)), ToTensor()]),\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.1098, 0.1098, 0.1098,  ..., 0.1255, 0.1176, 0.1137],\n",
       "           [0.1137, 0.1176, 0.1176,  ..., 0.1137, 0.1137, 0.1176],\n",
       "           [0.1216, 0.1255, 0.1216,  ..., 0.1098, 0.1176, 0.1137],\n",
       "           ...,\n",
       "           [0.1765, 0.1725, 0.1765,  ..., 0.0863, 0.0902, 0.0941],\n",
       "           [0.1686, 0.1686, 0.1608,  ..., 0.0902, 0.0902, 0.0902],\n",
       "           [0.1529, 0.1529, 0.1490,  ..., 0.0824, 0.0863, 0.0863]],\n",
       " \n",
       "          [[0.0549, 0.0549, 0.0549,  ..., 0.0824, 0.0745, 0.0706],\n",
       "           [0.0588, 0.0627, 0.0627,  ..., 0.0706, 0.0706, 0.0745],\n",
       "           [0.0627, 0.0667, 0.0627,  ..., 0.0667, 0.0745, 0.0706],\n",
       "           ...,\n",
       "           [0.2314, 0.2314, 0.2353,  ..., 0.1020, 0.1059, 0.1098],\n",
       "           [0.2314, 0.2314, 0.2275,  ..., 0.1059, 0.1059, 0.1059],\n",
       "           [0.2235, 0.2235, 0.2196,  ..., 0.0980, 0.1020, 0.1020]],\n",
       " \n",
       "          [[0.0196, 0.0196, 0.0196,  ..., 0.1059, 0.0980, 0.0941],\n",
       "           [0.0196, 0.0235, 0.0235,  ..., 0.0941, 0.0941, 0.0980],\n",
       "           [0.0196, 0.0235, 0.0235,  ..., 0.0902, 0.0980, 0.0941],\n",
       "           ...,\n",
       "           [0.1804, 0.1804, 0.1843,  ..., 0.1059, 0.1098, 0.1176],\n",
       "           [0.1765, 0.1804, 0.1765,  ..., 0.1098, 0.1098, 0.1137],\n",
       "           [0.1686, 0.1686, 0.1647,  ..., 0.1020, 0.1059, 0.1059]]]]),\n",
       " tensor([0])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = DataLoader(\n",
    "    dataset=train_data,\n",
    "    shuffle=False,\n",
    "    batch_size=1\n",
    ")\n",
    "next(iter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 225\n",
       "    Root location: data/images/train/\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 75)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(directory):\n",
    "    classes = sorted([x.name for x in list(os.scandir(directory))])\n",
    "    if classes:\n",
    "        class_to_idx = {value : idx for idx, value in enumerate(classes)}\n",
    "    else :\n",
    "        raise FileExistsError(\"No Classes found\")\n",
    "    return classes, class_to_idx\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class Custom_ImageFolder(Dataset):\n",
    "    from PIL import Image\n",
    "    from typing import Tuple, Dict,List\n",
    "    from torchvision.transforms import PILToTensor\n",
    "    ## Dunder Methods\n",
    "    def __init__(self, root:str, transformer= None,)-> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.paths = list(pathlib.Path(root).glob('*/*.jpg'))\n",
    "        self.transform = transformer\n",
    "        \n",
    "        self.classes, self.class_to_idx = find_classes(root)\n",
    "        \n",
    "    def __len__(self)->int:\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index:int)->Tuple[torch.Tensor, int]:\n",
    "        \n",
    "        img = self.load_image(index)\n",
    "        img_label =self.paths[index].parent.name\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(img), img_label\n",
    "        else:\n",
    "            return img, img_label\n",
    "    \n",
    "    ## Custom Methods\n",
    "    def return_path(self):\n",
    "        return self.paths\n",
    "    \n",
    "    def load_image(self, index:int)->Image.Image:\n",
    "        img_path = self.paths[index]\n",
    "        img = Image.open(img_path)\n",
    "        return img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_traindata = Custom_ImageFolder(\n",
    "    root=train_dir,\n",
    "    transformer=Compose([Resize((224,224)), ToTensor()])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "custom_train_data = DataLoader(\n",
    "    dataset=custom_traindata,\n",
    "    num_workers=0,\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.2902, 0.2863, 0.2941,  ..., 0.0039, 0.0118, 0.0078],\n",
       "           [0.2980, 0.3020, 0.2980,  ..., 0.0000, 0.0078, 0.0039],\n",
       "           [0.3137, 0.3059, 0.3020,  ..., 0.0000, 0.0039, 0.0039],\n",
       "           ...,\n",
       "           [0.1098, 0.0941, 0.1137,  ..., 0.8471, 0.8431, 0.8471],\n",
       "           [0.0941, 0.1098, 0.0980,  ..., 0.8431, 0.8392, 0.8392],\n",
       "           [0.0863, 0.0941, 0.0941,  ..., 0.8510, 0.8431, 0.8314]],\n",
       " \n",
       "          [[0.2353, 0.2353, 0.2431,  ..., 0.0235, 0.0314, 0.0275],\n",
       "           [0.2431, 0.2471, 0.2392,  ..., 0.0196, 0.0275, 0.0235],\n",
       "           [0.2549, 0.2471, 0.2431,  ..., 0.0196, 0.0235, 0.0235],\n",
       "           ...,\n",
       "           [0.1098, 0.0941, 0.1137,  ..., 0.8118, 0.8118, 0.8157],\n",
       "           [0.0941, 0.1098, 0.0980,  ..., 0.8118, 0.8078, 0.8078],\n",
       "           [0.0863, 0.0941, 0.0941,  ..., 0.8157, 0.8118, 0.8000]],\n",
       " \n",
       "          [[0.1255, 0.1373, 0.1608,  ..., 0.0078, 0.0157, 0.0118],\n",
       "           [0.1333, 0.1490, 0.1647,  ..., 0.0039, 0.0118, 0.0078],\n",
       "           [0.1451, 0.1529, 0.1686,  ..., 0.0039, 0.0078, 0.0078],\n",
       "           ...,\n",
       "           [0.0745, 0.0627, 0.0784,  ..., 0.8118, 0.8039, 0.8039],\n",
       "           [0.0510, 0.0667, 0.0588,  ..., 0.8157, 0.8039, 0.8000],\n",
       "           [0.0392, 0.0471, 0.0471,  ..., 0.8196, 0.8039, 0.7922]]]]),\n",
       " ('sushi',)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(custom_train_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reloading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "simple_transfrom = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data_simple  = datasets.ImageFolder(\n",
    "    root=train_dir, transform=simple_transfrom, target_transform=None)\n",
    "\n",
    "test_data_simple = datasets.ImageFolder(\n",
    "    root=test_dir, transform=simple_transfrom, target_transform=None)\n",
    "\n",
    "BATCH_SIZE =2 \n",
    "NUM_WORKERS = 4\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset= train_data_simple,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    #num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset= test_data_simple,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    #num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_train))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, n_channels, n_filter, n_classes)->None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=n_filter, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=n_filter, out_channels=n_filter, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(stride=2, kernel_size=2)\n",
    "            ).to(device)\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(n_filter, n_filter, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_filter, n_filter, stride=1, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=10*56*56, out_features=3),\n",
    "            #nn.Softmax(dim=1)\n",
    "        ).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        #x = x.argmax(dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = TinyVGG(\n",
    "    n_channels=3,\n",
    "    n_filter=10,\n",
    "    n_classes=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 56, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0057,  0.0328, -0.0101],\n",
       "        [-0.0097,  0.0384, -0.0135]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs , label = next(iter(dataloader_train))\n",
    "imgs\n",
    "# torch.softmax(vgg(imgs), dim=1).argmax(dim=1), label\n",
    "vgg(imgs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\dell\\anaconda3\\envs\\torch\\lib\\site-packages (1.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 56, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyVGG                                  [32, 3]                   --\n",
       "├─Sequential: 1-1                        [32, 10, 112, 112]        --\n",
       "│    └─Conv2d: 2-1                       [32, 10, 224, 224]        280\n",
       "│    └─ReLU: 2-2                         [32, 10, 224, 224]        --\n",
       "│    └─Conv2d: 2-3                       [32, 10, 224, 224]        910\n",
       "│    └─ReLU: 2-4                         [32, 10, 224, 224]        --\n",
       "│    └─MaxPool2d: 2-5                    [32, 10, 112, 112]        --\n",
       "├─Sequential: 1-2                        [32, 10, 56, 56]          --\n",
       "│    └─Conv2d: 2-6                       [32, 10, 112, 112]        910\n",
       "│    └─ReLU: 2-7                         [32, 10, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [32, 10, 112, 112]        910\n",
       "│    └─ReLU: 2-9                         [32, 10, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [32, 10, 56, 56]          --\n",
       "├─Sequential: 1-3                        [32, 3]                   --\n",
       "│    └─Flatten: 2-11                     [32, 31360]               --\n",
       "│    └─Linear: 2-12                      [32, 3]                   94,083\n",
       "==========================================================================================\n",
       "Total params: 97,093\n",
       "Trainable params: 97,093\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.64\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 321.13\n",
       "Params size (MB): 0.39\n",
       "Estimated Total Size (MB): 340.78\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "summary(vgg, input_size=[32,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training_model_fun import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fun(y_pred, y_true):\n",
    "    correct = torch.eq(y_pred,y_true).sum().item()\n",
    "    return (correct / len(y_true))*100\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(\n",
    "    params= vgg.parameters(),\n",
    "    lr = 0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([1, 10, 56, 56])\n",
      "Epoch: 2\n",
      "------ train Acc = 35.84% || train Loss = 1.10787\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([1, 10, 56, 56])\n",
      "------ test Acc = 25.00% || test Loss = 1.10536\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([1, 10, 56, 56])\n",
      "Epoch: 3\n",
      "------ train Acc = 46.46% || train Loss = 1.06371\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([1, 10, 56, 56])\n",
      "------ test Acc = 25.00% || test Loss = 1.32809\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n",
      "torch.Size([2, 10, 56, 56])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(\n\u001b[0;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mvgg,\n\u001b[0;32m      3\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mdataloader_train,\n\u001b[0;32m      4\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mdataloader_test,\n\u001b[0;32m      5\u001b[0m     loss_function\u001b[39m=\u001b[39;49mloss_function,\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[39m=\u001b[39;49mopt,\n\u001b[0;32m      7\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m      9\u001b[0m     accuracy_function\u001b[39m=\u001b[39;49maccuracy_fun,\n\u001b[0;32m     10\u001b[0m     show_every\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m     show_progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Desktop\\PyTorch\\04_custom_datasets\\utils\\training_model_fun.py:101\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, loss_function, optimizer, accuracy_function, epochs, show_progress, show_every, device)\u001b[0m\n\u001b[0;32m     94\u001b[0m results \u001b[39m=\u001b[39m {\n\u001b[0;32m     95\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTrain_loss\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     96\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTrain_acc\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTest_loss\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     98\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTest_acc\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     99\u001b[0m }\n\u001b[0;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m--> 101\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step(\n\u001b[0;32m    102\u001b[0m         model \u001b[39m=\u001b[39;49m model,\n\u001b[0;32m    103\u001b[0m         data_loader\u001b[39m=\u001b[39;49m train_dataloader,\n\u001b[0;32m    104\u001b[0m         loss_fn\u001b[39m=\u001b[39;49m loss_function,\n\u001b[0;32m    105\u001b[0m         optimizer\u001b[39m=\u001b[39;49m optimizer,\n\u001b[0;32m    106\u001b[0m         accuracy_fn\u001b[39m=\u001b[39;49m accuracy_function,\n\u001b[0;32m    107\u001b[0m         epoch \u001b[39m=\u001b[39;49m epoch \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m    108\u001b[0m         show_progress\u001b[39m=\u001b[39;49m show_progress,\n\u001b[0;32m    109\u001b[0m         show_every \u001b[39m=\u001b[39;49m show_every,\n\u001b[0;32m    110\u001b[0m         device\u001b[39m=\u001b[39;49m device,   \n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    113\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(\n\u001b[0;32m    114\u001b[0m         model \u001b[39m=\u001b[39m model,\n\u001b[0;32m    115\u001b[0m         data_loader\u001b[39m=\u001b[39m test_dataloader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m         device\u001b[39m=\u001b[39m device,   \n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     results[\u001b[39m\"\u001b[39m\u001b[39mTrain_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Desktop\\PyTorch\\04_custom_datasets\\utils\\training_model_fun.py:20\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data_loader, loss_fn, optimizer, accuracy_fn, epoch, show_progress, show_every, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m loss_train, acc_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m     18\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[0;32m     21\u001b[0m     x, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m     logits \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 230\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\folder.py:249\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    248\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\PIL\\Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[0;32m    874\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m    875\u001b[0m ):\n\u001b[0;32m    876\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    923\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    924\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    925\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\PIL\\ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    259\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 260\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    262\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=vgg,\n",
    "    train_dataloader=dataloader_train,\n",
    "    test_dataloader=dataloader_test,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=opt,\n",
    "    epochs=10,\n",
    "    device=device,\n",
    "    accuracy_function=accuracy_fun,\n",
    "    show_every=1,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9c3c500b8bab2cdd86682acf135365680fb2fcb10592a31bb294351b9c146c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
