{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generatror(nn.Module):\n",
    "    def __init__(self, z_dim: int, img_dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.gen_block1 = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.gen_block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.gen_block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.gen_block4 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 8),\n",
    "            nn.BatchNorm1d(hidden_dim * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.last_block = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 8, img_dim), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gen_block1(x)\n",
    "        x = self.gen_block2(x)\n",
    "        x = self.gen_block3(x)\n",
    "        x = self.gen_block4(x)\n",
    "        x = self.last_block(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generatror(\n",
    "    z_dim=10,\n",
    "    img_dim = 784,\n",
    "    hidden_dim = 64).to('cuda')\n",
    "next(gen.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gen(torch.randn(1, 10, device=\"cuda\"))\n",
    "    #! Expected more than 1 value per channel when training, got input size torch.Size([1, 128])\n",
    "except:\n",
    "    print(\"input more than 1 sample\")\n",
    "# raise ValueError('input more than 1 sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_img = gen(torch.randn(2,10,device='cuda'))[1]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "generated_img = generated_img.to('cpu').detach().numpy()\n",
    "plt.imshow(generated_img.reshape(28,28), cmap='gray')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim, img_dim) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.disc_block1 = nn.Sequential(\n",
    "            nn.Linear(img_dim, hidden_dim*8),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.disc_block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*8, hidden_dim*4),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.disc_block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*4, hidden_dim*2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.disc_block4 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.linear_classifier = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.disc_block1(x)\n",
    "        x = self.disc_block2(x)\n",
    "        x = self.disc_block3(x)\n",
    "        x = self.disc_block4(x)\n",
    "        x = self.linear_classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(\n",
    "    hidden_dim=64,\n",
    "    img_dim=784\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img = torch.rand(1,28*28,device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc(rand_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your parameters\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 10\n",
    "img_dim = 28*28\n",
    "hidden_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 64\n",
    "lr = 0.00001\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load MNIST dataset as tensors\n",
    "dataloader = DataLoader(\n",
    "    MNIST('.', download=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "### DO NOT EDIT ###\n",
    "device = 'cuda'\n",
    "\n",
    "gen = Generatror(z_dim= z_dim, img_dim=img_dim, hidden_dim=hidden_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "disc =Discriminator(img_dim=img_dim, hidden_dim=hidden_dim).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(1000)\n",
    "noise = torch.randn(batch_size, z_dim, device= device)\n",
    "real_img = torch.randn(batch_size, img_dim, device= device)\n",
    "real_img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(disc.parameters()).is_cuda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### disc loss\n",
    "\n",
    "$$ L_{disc} = - \\frac{1}{N} \\sum_{i=1}^N \\left[ \\log D(x_i) + \\log (1 - D(G(z_i))) \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_disc_loss(disc, gen, criterion, real_img, batch_size, z_dim, device):\n",
    "    ## claculated Gen loss\n",
    "    #* noise or Z\n",
    "    noise = torch.randn(batch_size, z_dim, device= device)\n",
    "    #real_img = torch.randn(batch_size, img_dim, device= device)\n",
    "\n",
    "    #*1 generate fake nosie \n",
    "    gen_fake = gen(noise)\n",
    "    #*2 pass the generated img to the disc\n",
    "        ## detaching to make the generated fake img, un-removed when doing the back prop\n",
    "    disc_fake = disc(gen_fake.detach())\n",
    "    #*3 calculated losses \n",
    "        ## fake images disc loss\n",
    "        ## real images disc loss\n",
    "        \n",
    "        #*3.1 fake disc loss\n",
    "        #* disc fake loss compare disc fake with torch of zeros like \n",
    "        #* D(G(z))\n",
    "        \n",
    "    disc_fake_loss = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "    disc_real = disc(real_img)\n",
    "\n",
    "        #*3.2 real disc loss\n",
    "        #* disc real loss compare disc real with torch of ones like\n",
    "        #* D(x)\n",
    "        \n",
    "    disc_real_loss = criterion(disc_real, torch.ones_like(disc_real))\n",
    "    Disc_LOSS = (disc_fake_loss+disc_real_loss)/2\n",
    "\n",
    "    return Disc_LOSS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_disc_loss(\n",
    "    device=device,\n",
    "    disc=disc,\n",
    "    gen=gen,\n",
    "    criterion=criterion,\n",
    "    real_img= real_img,\n",
    "    z_dim=z_dim,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator loss\n",
    "\n",
    "$$ L_{gen} = - \\frac{1}{N} \\sum_{i=1}^N \\log D(G(z_i)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_loss(gen, disc, criterion, batch_size, z_dim, device):\n",
    "\n",
    "    noise = torch.randn(batch_size, z_dim, device=device)\n",
    "    generated_img = gen(noise)\n",
    "    disc_generated = disc(generated_img)\n",
    "    Gen_LOSS = criterion(disc_generated, torch.ones_like(disc_generated))\n",
    "\n",
    "    return Gen_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gen_loss(\n",
    "    gen=gen,\n",
    "    disc= disc,\n",
    "    criterion=criterion,\n",
    "    batch_size=batch_size,\n",
    "    z_dim=z_dim,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloader))\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts = len(images) # 64 \n",
    "images.view(-1).shape, images.view(bts, -1).shape, images.shape\n",
    "\n",
    "## (torch.Size([50176]), torch.Size([64, 784]), torch.Size([64, 1, 28, 28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step = 0\n",
    "mean_gen_loss = 0\n",
    "mean_disc_loss = 0\n",
    "device = \"cuda\"\n",
    "n_epochs = 200\n",
    "display_step = 5\n",
    "\n",
    "test_generato = True\n",
    "generator_loss = False\n",
    "discriminator = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for real, _ in tqdm(dataloader):\n",
    "\n",
    "        current_batch_size = len(real)\n",
    "\n",
    "        ## reshaping\n",
    "        # * reshape tensor from (64,1,28,28) to (64, 784)\n",
    "        real = real.view(current_batch_size, -1).to(device)\n",
    "\n",
    "        disc_opt.zero_grad()\n",
    "        disc_loss = get_disc_loss(\n",
    "            gen=gen,\n",
    "            disc=disc,\n",
    "            criterion=criterion,\n",
    "            real_img=real,\n",
    "            batch_size=current_batch_size,\n",
    "            z_dim=z_dim,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step()\n",
    "        # print(disc_loss)\n",
    "\n",
    "        ## Tracking generator weights\n",
    "        # * could work if i have only forward once\n",
    "        # * but since i have more than 1 block i can't track this way\n",
    "        # * sol is to track each block alone\n",
    "        # old_generator_weights = gen.gen[0][0].weight.detach().clone()\n",
    "        # old_generator_weights = gen.gen[0][0].weight.detach().clone()\n",
    "        # old_generator_weights = gen.gen[0][0].weight.detach().clone()\n",
    "\n",
    "        ## Updating the Generator\n",
    "        gen_opt.zero_grad()\n",
    "        gen_loss = get_gen_loss(\n",
    "            gen=gen,\n",
    "            disc=disc,\n",
    "            criterion=criterion,\n",
    "            batch_size=current_batch_size,\n",
    "            z_dim=z_dim,\n",
    "            device=device,\n",
    "        )\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        \n",
    "        ## \n",
    "        mean_disc_loss += disc_loss.item()\n",
    "        mean_gen_loss += gen_loss.item()\n",
    "        \n",
    "    mean_disc_loss = mean_disc_loss / current_batch_size\n",
    "    mean_gen_loss = mean_gen_loss / current_batch_size\n",
    "    \n",
    "    \n",
    "    gen.eval()\n",
    "    gen_fake = gen(noise)\n",
    "    print(f\"Epoch {epoch} : Generator Loss : {mean_gen_loss/len(dataloader)} Discriminator Loss : {mean_disc_loss/len(dataloader)}\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_imgs = gen(noise)\n",
    "generated_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 12 images from the generated images with matplotlib\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=6, sharex=True, sharey=True, figsize=(24,6))\n",
    "# for ax, img in zip(axes.flatten(), generated_imgs):\n",
    "#     img = img.detach().cpu().numpy()\n",
    "#     ax.xaxis.set_visible(False)\n",
    "#     ax.yaxis.set_visible(False)\n",
    "#     im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9c3c500b8bab2cdd86682acf135365680fb2fcb10592a31bb294351b9c146c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
