{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn , optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, img_dim),\n",
    "            #nn.Tanh()\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 16\n",
    "image_dim = 28*28*1\n",
    "batch_size = 4\n",
    "num_epochs = 2\n",
    "\n",
    "## creating the model\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device) #* for testing\n",
    "\n",
    "## Creating the trasnforms\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "    (2): Linear(in_features=128, out_features=784, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the dataset\n",
    "dataset  = datasets.MNIST(\n",
    "    root= \"dataset/\",\n",
    "    transform=transforms,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss and optimizer\n",
    "\n",
    "opt_disc = optim.Adam(disc.parameters(), lr= lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr= lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorboard\n",
    "\n",
    "# writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
    "# writer_real = SummaryWriter(f\"runs/GAN_MNIST/real\")\n",
    "# step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2455,  0.0692, -1.3365, -0.2672, -0.1551, -0.2727,  0.3241,  0.1741,\n",
       "          1.5673, -0.6120,  1.8356, -1.1575, -0.2227,  0.6297, -2.4234,  0.4870],\n",
       "        [-0.2768, -0.6736, -2.2939, -0.5892, -1.1272,  0.0913, -1.7003,  0.2753,\n",
       "         -0.5953,  1.2793, -1.2535, -1.1080, -0.4976,  0.8704,  1.5902,  1.2096],\n",
       "        [-2.4683,  0.1226, -1.2422,  0.6586, -0.4295,  0.6612,  0.5049, -0.3468,\n",
       "          0.8434, -0.2277, -0.8666,  1.5982, -0.8927, -1.8239,  0.3297,  0.6832],\n",
       "        [-0.2576,  0.9524,  0.4714,  0.9698, -1.1119, -1.2335,  0.0649,  0.2261,\n",
       "          0.1460, -1.2439, -1.4365,  1.2230, -0.2849, -0.1662, -0.7894,  0.4462]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        #\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "        \n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        #* \n",
    "        \n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        \n",
    "        \n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        \n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6114, 0.4754, 0.4116, 0.4589, 0.4259, 0.3793, 0.4241, 0.5815, 0.4967,\n",
      "         0.5552, 0.4638, 0.5414, 0.5225, 0.5568, 0.4629, 0.4707, 0.5518, 0.4813,\n",
      "         0.5954, 0.5024, 0.4019, 0.5248, 0.5120, 0.4481, 0.4732, 0.3514, 0.4941,\n",
      "         0.5482, 0.4941, 0.4761, 0.4360, 0.4404, 0.6296, 0.4801, 0.5077, 0.4938,\n",
      "         0.5698, 0.4963, 0.4856, 0.4555, 0.4971, 0.5313, 0.5889, 0.5020, 0.5505,\n",
      "         0.5475, 0.3688, 0.5853, 0.4621, 0.4411, 0.5350, 0.4753, 0.5172, 0.5200,\n",
      "         0.4688, 0.4287, 0.4640, 0.5593, 0.5872, 0.4705, 0.5208, 0.5932, 0.5931,\n",
      "         0.4964, 0.5514, 0.5012, 0.5354, 0.3962, 0.5685, 0.4883, 0.4482, 0.5479,\n",
      "         0.5057, 0.3928, 0.4911, 0.5538, 0.6254, 0.5168, 0.5749, 0.5659, 0.4511,\n",
      "         0.5307, 0.4597, 0.5721, 0.5198, 0.6467, 0.5374, 0.4817, 0.5435, 0.6040,\n",
      "         0.4249, 0.5923, 0.5161, 0.5288, 0.5027, 0.3562, 0.5244, 0.4063, 0.5166,\n",
      "         0.6101, 0.5153, 0.5044, 0.5374, 0.5459, 0.4352, 0.3775, 0.4792, 0.5298,\n",
      "         0.5532, 0.5520, 0.4918, 0.5170, 0.4424, 0.4398, 0.5163, 0.4904, 0.4520,\n",
      "         0.4844, 0.4851, 0.3963, 0.5640, 0.6179, 0.4876, 0.4765, 0.5435, 0.4718,\n",
      "         0.5685, 0.4920, 0.5173, 0.4601, 0.4974, 0.5038, 0.5334, 0.4988, 0.5280,\n",
      "         0.4696, 0.4760, 0.4164, 0.5738, 0.5177, 0.4801, 0.5166, 0.4585, 0.4297,\n",
      "         0.4606, 0.4691, 0.5318, 0.5054, 0.4682, 0.4636, 0.5046, 0.5331, 0.5020,\n",
      "         0.5769, 0.4982, 0.4519, 0.5254, 0.5360, 0.5669, 0.5172, 0.5220, 0.4244,\n",
      "         0.5048, 0.5611, 0.5717, 0.4144, 0.5518, 0.4604, 0.5893, 0.4164, 0.4825,\n",
      "         0.5216, 0.5140, 0.4418, 0.5907, 0.4702, 0.5365, 0.5213, 0.4902, 0.5505,\n",
      "         0.5080, 0.5270, 0.4586, 0.4981, 0.4636, 0.5222, 0.5274, 0.4678, 0.6046,\n",
      "         0.5259, 0.4898, 0.4359, 0.5101, 0.4217, 0.4717, 0.4273, 0.4763, 0.3917,\n",
      "         0.4731, 0.4507, 0.5683, 0.5649, 0.4085, 0.4819, 0.5417, 0.4364, 0.4525,\n",
      "         0.4798, 0.4904, 0.5411, 0.5206, 0.4669, 0.5224, 0.4998, 0.5140, 0.5522,\n",
      "         0.5452, 0.4533, 0.4283, 0.4729, 0.5156, 0.4318, 0.5122, 0.5202, 0.4746,\n",
      "         0.5211, 0.5452, 0.4822, 0.5066, 0.4984, 0.4907, 0.4297, 0.4406, 0.5315,\n",
      "         0.4541, 0.4601, 0.5332, 0.4702, 0.5898, 0.4011, 0.5217, 0.4816, 0.5052,\n",
      "         0.5402, 0.5342, 0.4587, 0.4826, 0.5599, 0.4187, 0.4821, 0.5618, 0.4378,\n",
      "         0.4629, 0.4885, 0.5373, 0.4373, 0.4913, 0.5240, 0.5249, 0.4798, 0.5123,\n",
      "         0.4845, 0.5627, 0.4586, 0.5040, 0.4798, 0.4997, 0.5236, 0.5148, 0.4399,\n",
      "         0.4772, 0.5166, 0.4320, 0.4320, 0.4812, 0.5586, 0.5537, 0.4875, 0.5738,\n",
      "         0.4768, 0.4887, 0.5864, 0.4377, 0.6039, 0.3864, 0.4179, 0.5111, 0.4763,\n",
      "         0.4883, 0.6374, 0.5331, 0.4988, 0.4852, 0.4973, 0.5749, 0.4970, 0.4256,\n",
      "         0.4222, 0.5576, 0.4700, 0.4614, 0.4895, 0.4516, 0.5511, 0.4222, 0.5518,\n",
      "         0.5616, 0.5193, 0.5526, 0.5901, 0.4710, 0.4640, 0.5893, 0.6389, 0.5001,\n",
      "         0.5465, 0.5223, 0.4051, 0.5285, 0.4560, 0.5854, 0.4597, 0.5352, 0.5159,\n",
      "         0.6463, 0.5739, 0.5165, 0.5239, 0.3215, 0.5960, 0.4749, 0.5699, 0.4988,\n",
      "         0.4947, 0.4235, 0.5487, 0.5050, 0.6338, 0.4599, 0.5639, 0.5647, 0.6253,\n",
      "         0.5453, 0.3969, 0.5769, 0.5473, 0.5191, 0.4853, 0.4986, 0.4314, 0.5887,\n",
      "         0.4964, 0.4751, 0.4984, 0.6100, 0.5059, 0.5747, 0.4671, 0.5703, 0.3131,\n",
      "         0.4512, 0.6170, 0.4395, 0.5506, 0.4762, 0.4568, 0.4638, 0.4310, 0.5621,\n",
      "         0.5104, 0.5165, 0.4547, 0.4857, 0.4308, 0.4976, 0.4976, 0.3870, 0.4931,\n",
      "         0.5442, 0.5324, 0.5078, 0.4978, 0.4006, 0.5366, 0.4233, 0.6122, 0.4991,\n",
      "         0.4785, 0.4926, 0.5328, 0.4939, 0.5672, 0.4247, 0.5197, 0.4460, 0.4239,\n",
      "         0.5331, 0.4969, 0.4169, 0.4804, 0.5245, 0.4683, 0.6340, 0.6536, 0.3911,\n",
      "         0.5830, 0.4813, 0.6022, 0.5466, 0.5358, 0.5932, 0.5007, 0.4703, 0.4770,\n",
      "         0.5212, 0.5302, 0.4931, 0.4440, 0.5073, 0.6065, 0.4973, 0.5074, 0.4888,\n",
      "         0.5246, 0.4542, 0.4376, 0.5412, 0.4854, 0.5148, 0.5398, 0.5195, 0.6061,\n",
      "         0.5908, 0.4899, 0.4602, 0.4474, 0.5425, 0.5897, 0.5828, 0.5080, 0.4761,\n",
      "         0.5980, 0.5025, 0.5501, 0.3811, 0.5484, 0.5439, 0.4805, 0.5324, 0.5347,\n",
      "         0.5459, 0.5271, 0.3994, 0.4691, 0.4529, 0.4731, 0.4610, 0.5254, 0.4299,\n",
      "         0.5709, 0.4318, 0.4518, 0.4919, 0.5528, 0.5246, 0.4845, 0.5215, 0.5366,\n",
      "         0.5210, 0.4335, 0.5106, 0.4777, 0.4654, 0.5287, 0.5650, 0.5616, 0.5793,\n",
      "         0.4900, 0.4063, 0.4646, 0.4740, 0.5248, 0.5690, 0.4946, 0.5778, 0.4763,\n",
      "         0.4972, 0.4375, 0.4963, 0.5392, 0.5086, 0.4634, 0.5837, 0.5544, 0.5771,\n",
      "         0.4374, 0.5404, 0.4378, 0.5542, 0.4701, 0.4896, 0.5764, 0.5315, 0.5726,\n",
      "         0.4662, 0.5448, 0.4691, 0.4807, 0.5297, 0.5934, 0.5037, 0.5659, 0.4408,\n",
      "         0.5595, 0.4082, 0.5128, 0.5052, 0.4082, 0.4914, 0.5155, 0.5208, 0.4561,\n",
      "         0.4751, 0.4240, 0.5369, 0.5600, 0.5750, 0.4598, 0.4346, 0.5914, 0.5769,\n",
      "         0.3742, 0.4049, 0.3481, 0.4636, 0.4697, 0.5208, 0.5012, 0.5873, 0.4582,\n",
      "         0.4632, 0.4619, 0.4544, 0.4860, 0.5591, 0.4431, 0.4514, 0.4752, 0.4245,\n",
      "         0.5278, 0.4994, 0.4636, 0.4686, 0.4255, 0.5055, 0.4490, 0.5721, 0.5580,\n",
      "         0.4565, 0.5516, 0.4955, 0.4635, 0.4725, 0.5827, 0.6094, 0.4287, 0.5302,\n",
      "         0.4916, 0.5828, 0.4525, 0.4708, 0.4732, 0.4919, 0.5569, 0.5032, 0.4912,\n",
      "         0.4781, 0.5555, 0.5199, 0.5123, 0.5400, 0.5676, 0.5163, 0.5484, 0.4852,\n",
      "         0.5046, 0.4902, 0.4909, 0.4581, 0.4791, 0.5326, 0.5683, 0.4600, 0.5588,\n",
      "         0.5371, 0.5327, 0.4600, 0.5153, 0.4455, 0.4475, 0.4693, 0.4766, 0.5189,\n",
      "         0.4430, 0.4426, 0.5289, 0.4956, 0.5701, 0.4376, 0.5077, 0.4798, 0.6289,\n",
      "         0.5886, 0.4922, 0.4722, 0.4985, 0.3836, 0.4788, 0.4929, 0.5761, 0.5247,\n",
      "         0.4310, 0.5976, 0.4992, 0.5179, 0.4140, 0.4918, 0.5122, 0.5623, 0.5994,\n",
      "         0.5376, 0.5777, 0.4392, 0.4216, 0.5701, 0.4421, 0.5580, 0.5440, 0.4600,\n",
      "         0.4429, 0.5504, 0.4366, 0.4684, 0.6201, 0.4232, 0.5156, 0.5242, 0.4501,\n",
      "         0.5195, 0.4628, 0.4820, 0.5998, 0.4934, 0.5276, 0.5802, 0.4757, 0.4515,\n",
      "         0.5419, 0.5651, 0.5405, 0.5082, 0.6559, 0.5312, 0.6025, 0.4991, 0.5676,\n",
      "         0.5124, 0.5619, 0.5439, 0.3913, 0.5007, 0.5326, 0.4750, 0.5020, 0.4687,\n",
      "         0.4299, 0.5402, 0.5218, 0.4768, 0.4324, 0.3933, 0.5375, 0.4166, 0.5993,\n",
      "         0.5350, 0.5149, 0.4316, 0.4182, 0.5241, 0.4874, 0.5172, 0.4877, 0.5092,\n",
      "         0.5669, 0.4820, 0.4207, 0.4082, 0.5512, 0.4383, 0.4558, 0.5942, 0.5273,\n",
      "         0.5240, 0.5185, 0.4089, 0.5917, 0.4743, 0.5131, 0.4649, 0.4873, 0.5307,\n",
      "         0.4729, 0.4834, 0.5356, 0.5459, 0.4662, 0.4886, 0.4460, 0.5300, 0.5205,\n",
      "         0.5160, 0.4956, 0.5262, 0.4495, 0.5067, 0.5658, 0.5408, 0.4577, 0.5403,\n",
      "         0.4792, 0.4805, 0.5269, 0.5306, 0.3710, 0.5759, 0.5070, 0.4676, 0.5334,\n",
      "         0.5213, 0.4175, 0.5326, 0.4679, 0.5084, 0.4922, 0.6753, 0.4751, 0.5737,\n",
      "         0.4667, 0.5346, 0.5663, 0.4980, 0.4765, 0.5316, 0.5031, 0.5389, 0.5010,\n",
      "         0.5115, 0.3437, 0.5399, 0.4683, 0.4602, 0.3946, 0.5264, 0.4071, 0.5585,\n",
      "         0.4986, 0.4796, 0.5342, 0.5151, 0.4638, 0.6018, 0.4486, 0.4656, 0.5313,\n",
      "         0.4704, 0.5069, 0.4569, 0.5469, 0.5011, 0.5317, 0.4719, 0.5691, 0.5393,\n",
      "         0.4088]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m disc_real \u001b[39m=\u001b[39m real\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39m#print(disc_real)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m lossD_real \u001b[39m=\u001b[39m criterion(disc_real, torch\u001b[39m.\u001b[39;49mones_like(disc_real))\n\u001b[0;32m     21\u001b[0m \u001b[39m#disc_fake = disc(fake).view(-1)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m disc_fake \u001b[39m=\u001b[39m disc(fake)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:613\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:3083\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3080\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3081\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3083\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx , (real, _) in enumerate(loader):\n",
    "        #* view(-1) is used to flatten the image. instead of `torch.flatten`\n",
    "        real = real.view(-1,784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "        \n",
    "        ## training the Discriminator\n",
    "        #* train disc = max [log(D(real)) + log(1-D(G(noise)))]\n",
    "            \n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)        \n",
    "        disc_real = real.view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "            \n",
    "        \n",
    "        #disc_fake = disc(fake).view(-1)\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        \n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        ## training the generator\n",
    "        #* train gen = min [log(1-D(G(noise)))] or max [log(D(G(noise))]\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG  = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        \n",
    "        # ## Tensorboard\n",
    "        # if batch_idx == 0:\n",
    "        #     print(f\"Epoch [{epoch}/{num_epochs}]\" \\\n",
    "        #         f\"Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\")\n",
    "            \n",
    "        #     with torch.inference_mode():\n",
    "        #         fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "        #         data = real.resape(-1, 1, 28, 28)\n",
    "        #         img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "        #         img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "                \n",
    "        #         writer_fake.add_image(\n",
    "        #             \"Mnist Fake Images\", img_grid_fake, global_step=step)\n",
    "\n",
    "        #         writer_real.add_image(\n",
    "        #             \"Mnist Real Images\", img_grid_real, global_step=step)\n",
    "        #         step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9c3c500b8bab2cdd86682acf135365680fb2fcb10592a31bb294351b9c146c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
